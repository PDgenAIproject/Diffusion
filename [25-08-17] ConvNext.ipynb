{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode, functional as TF\n",
    "\n",
    "import timm\n",
    "from timm.data import resolve_model_data_config\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LT_slit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- config ----------\n",
    "DATA_DIR = Path(\"/camin1/chlee/Data/cornea_image/segmented/Lt/slit/split\")  # <- 본인 경로로 변경\n",
    "TRAIN_DIR    = DATA_DIR / \"train\"\n",
    "VAL_DIR      = DATA_DIR / \"val\"\n",
    "\n",
    "NUM_CLASSES  = 4\n",
    "TARGET_SIZE  = 384                     # 모델 권장 입력\n",
    "BATCH_SIZE   = 8                       # OOM 나면 줄이세요\n",
    "EPOCHS       = 80\n",
    "LR_HEAD      = 3e-4\n",
    "LR_BACKBONE  = 1e-5\n",
    "WEIGHT_DECAY = 0.05\n",
    "UNFREEZE_STAGES = 2                    # 마지막 stage 2개 학습(총 4개 중 3,4)\n",
    "NUM_WORKERS  = 4\n",
    "SEED         = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- model ----------\n",
    "MODEL_ID = \"convnextv2_base.fcmae_ft_in22k_in1k_384\"\n",
    "model = timm.create_model(MODEL_ID, pretrained=True, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (1): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): ConvNeXtStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (4): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (5): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (6): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (7): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (8): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (9): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (10): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (11): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (12): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (13): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (14): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (15): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (16): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (17): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (18): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (19): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (20): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (21): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (22): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (23): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (24): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (25): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (26): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_pre): Identity()\n",
       "  (head): NormMlpClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (norm): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (pre_logits): Identity()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=1024, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 안전하게 분류기 리셋\n",
    "if hasattr(model, \"reset_classifier\"):\n",
    "    model.reset_classifier(num_classes=NUM_CLASSES)\n",
    "model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- transforms (LongestSide + square pad) ----------\n",
    "cfg = resolve_model_data_config(model)  # mean/std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResizeLongestSide:\n",
    "    def __init__(self, size, interpolation=InterpolationMode.BICUBIC):\n",
    "        self.size = size; self.interp = interpolation\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        scale = self.size / max(w, h)\n",
    "        new_w, new_h = int(round(w*scale)), int(round(h*scale))\n",
    "        return TF.resize(img, (new_h, new_w), interpolation=self.interp, antialias=True)\n",
    "\n",
    "class PadToSquare:\n",
    "    def __init__(self, size, fill=(0,0,0)):\n",
    "        self.size = size; self.fill = fill\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        pw, ph = self.size - w, self.size - h\n",
    "        pad = (pw//2, ph//2, pw - pw//2, ph - ph//2)\n",
    "        return TF.pad(img, pad, fill=self.fill)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pad_fill = tuple(int(m*255) for m in cfg['mean'])  # 배경을 mean 색으로 패딩\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    ResizeLongestSide(TARGET_SIZE),\n",
    "    PadToSquare(TARGET_SIZE, fill=pad_fill),\n",
    "    # 의료: 강한 크롭/플립 제거, 약한 회전만\n",
    "    transforms.RandomApply([transforms.RandomRotation(5, interpolation=InterpolationMode.BILINEAR)], p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cfg['mean'], std=cfg['std']),\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    ResizeLongestSide(TARGET_SIZE),\n",
    "    PadToSquare(TARGET_SIZE, fill=pad_fill),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cfg['mean'], std=cfg['std']),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- datasets / loaders ----------\n",
    "train_ds = ImageFolder(TRAIN_DIR, transform=train_tf)\n",
    "val_ds   = ImageFolder(VAL_DIR,   transform=val_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train classes: {'0': 0, '1': 1, '2': 2, '3': 3}\n",
      "val classes: {'0': 0, '1': 1, '2': 2, '3': 3}\n",
      "train counts: Counter({2: 78, 0: 75, 3: 45, 1: 11})\n",
      "val counts: Counter({2: 56, 0: 45, 3: 27, 1: 14})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"train classes:\", train_ds.class_to_idx)\n",
    "print(\"val classes:\",   val_ds.class_to_idx)\n",
    "print(\"train counts:\",  Counter(train_ds.targets))\n",
    "print(\"val counts:\",    Counter(val_ds.targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor(train_ds.targets)\n",
    "class_counts = torch.bincount(targets, minlength=NUM_CLASSES).float()\n",
    "# 각 클래스 반비례 가중치(총합 정규화)\n",
    "weights = (class_counts.sum() / (class_counts + 1e-6)).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=weights,\n",
    "    label_smoothing=0.1,  # 작은 데이터에 도움\n",
    ").to(DEVICE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- partial fine-tune (head + last stages) ----------\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# head 활성화\n",
    "if hasattr(model, \"head\"):\n",
    "    for p in model.head.parameters():\n",
    "        p.requires_grad = True\n",
    "# 마지막 N개 stage 활성화\n",
    "if hasattr(model, \"stages\"):\n",
    "    total = len(model.stages)\n",
    "    start = max(0, total - UNFREEZE_STAGES)\n",
    "    for i in range(start, total):\n",
    "        for p in model.stages[i].parameters():\n",
    "            p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 파라미터 그룹 (head vs backbone)\n",
    "head_params, backbone_params = [], []\n",
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        if (\"head\" in n) or (\"classifier\" in n):\n",
    "            head_params.append(p)\n",
    "        else:\n",
    "            backbone_params.append(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNFREEZE_STAGES = 4  # 전부\n",
    "for p in model.parameters(): p.requires_grad = False\n",
    "for p in model.head.parameters(): p.requires_grad = True\n",
    "for i in range(len(model.stages) - UNFREEZE_STAGES, len(model.stages)):\n",
    "    for p in model.stages[i].parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": [p for n,p in model.named_parameters() if p.requires_grad and \"head\" not in n], \"lr\": 5e-5},\n",
    "    {\"params\": [p for n,p in model.named_parameters() if p.requires_grad and (\"head\" in n or \"classifier\" in n)], \"lr\": 3e-4},\n",
    "], weight_decay=0.01)\n",
    "\n",
    "\n",
    "\n",
    "num_train_steps = EPOCHS * len(train_loader)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_train_steps),\n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "scaler = torch.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- utils ----------\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for x, y in val_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        logits = model(x)\n",
    "        total_loss += criterion(logits, y).item() * x.size(0)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return total_loss / max(total, 1), correct / max(total, 1)\n",
    "\n",
    "\n",
    "def eval_confmat():\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            ps.extend(model(x).argmax(1).cpu().tolist())\n",
    "            ys.extend(y.tolist())\n",
    "    cm = confusion_matrix(ys, ps, labels=list(range(NUM_CLASSES)))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "def pred_hist():\n",
    "    model.eval()\n",
    "    ps = []\n",
    "    with torch.no_grad():\n",
    "        for x, _ in val_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            ps.extend(model(x).argmax(1).cpu().tolist())\n",
    "    hist = torch.bincount(torch.tensor(ps), minlength=NUM_CLASSES).tolist()\n",
    "    print(\"pred hist:\", hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 87690372\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------- training ----------\n",
    "print(\"trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chlee/.conda/envs/keratopathy/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] val_loss=1.2923  acc=0.3732\n",
      "[2] val_loss=1.3290  acc=0.3451\n",
      "[3] val_loss=1.3058  acc=0.3521\n",
      "[4] val_loss=1.2301  acc=0.4296\n",
      "[5] val_loss=1.3959  acc=0.4085\n",
      "[6] val_loss=1.7480  acc=0.4366\n",
      "[7] val_loss=1.8425  acc=0.4789\n",
      "[8] val_loss=2.6416  acc=0.3662\n",
      "[9] val_loss=2.7400  acc=0.2817\n",
      "[10] val_loss=2.0852  acc=0.3944\n",
      "[11] val_loss=1.8593  acc=0.3873\n",
      "[12] val_loss=2.9054  acc=0.4155\n",
      "[13] val_loss=2.7738  acc=0.4296\n",
      "[14] val_loss=2.2576  acc=0.3944\n",
      "[15] val_loss=2.5976  acc=0.3944\n",
      "[16] val_loss=2.6085  acc=0.4296\n",
      "[17] val_loss=2.5613  acc=0.4296\n",
      "[18] val_loss=2.7515  acc=0.4789\n",
      "[19] val_loss=3.5832  acc=0.3944\n",
      "⏹ Early stopped at epoch 19 (no improvement for 15 epochs).\n"
     ]
    }
   ],
   "source": [
    "# ==== EarlyStopping 파라미터 ====\n",
    "best_loss = float('inf')\n",
    "patience = 15         # 권장: 10~15\n",
    "min_delta = 1e-3      # 이만큼 내려가야 “개선”으로 인정\n",
    "no_improve = 0\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for step, (x, y) in enumerate(train_loader, 1):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        with torch.amp.autocast(device_type=DEVICE, enabled=(DEVICE==\"cuda\")):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()  # per-step 스케줄 유지\n",
    "\n",
    "        running += loss.item()\n",
    "        if step % 50 == 0:\n",
    "            print(f\"ep {ep} | step {step}/{len(train_loader)} | train_loss {running/50:.4f}\")\n",
    "            running = 0.0\n",
    "\n",
    "    val_loss, val_acc = evaluate()\n",
    "    print(f\"[{ep}] val_loss={val_loss:.4f}  acc={val_acc:.4f}\")\n",
    "\n",
    "    # 저장 & EarlyStopping 로직\n",
    "    if best_loss - val_loss > min_delta:\n",
    "        best_loss = val_loss\n",
    "        no_improve = 0\n",
    "        # torch.save(model.state_dict(), \"/camin1/chlee/jupyter/Keratopathy AI Project/[25-08-11]/convnextv2_base_384_cornea_best.pt\")\n",
    "        # print(\"💾 saved: convnextv2_base_384_cornea_best.pt\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"⏹ Early stopped at epoch {ep} (no improvement for {patience} epochs).\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LT_slit_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train classes: {'0': 0, '1': 1, '2': 2, '3': 3}\n",
      "val classes: {'0': 0, '1': 1, '2': 2, '3': 3}\n",
      "train counts: Counter({2: 63, 0: 59, 3: 43, 1: 9})\n",
      "val counts: Counter({2: 49, 0: 41, 3: 22, 1: 12})\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"/camin1/chlee/Data/cornea_image/segmented/Lt/slit_beam/split\")  # <- 본인 경로로 변경\n",
    "TRAIN_DIR    = DATA_DIR / \"train\"\n",
    "VAL_DIR      = DATA_DIR / \"val\"\n",
    "\n",
    "model = timm.create_model(MODEL_ID, pretrained=True, num_classes=NUM_CLASSES)\n",
    "\n",
    "# 안전하게 분류기 리셋\n",
    "if hasattr(model, \"reset_classifier\"):\n",
    "    model.reset_classifier(num_classes=NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "\n",
    "# ---------- transforms (LongestSide + square pad) ----------\n",
    "cfg = resolve_model_data_config(model)  # mean/std\n",
    "\n",
    "\n",
    "pad_fill = tuple(int(m*255) for m in cfg['mean'])  # 배경을 mean 색으로 패딩\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    ResizeLongestSide(TARGET_SIZE),\n",
    "    PadToSquare(TARGET_SIZE, fill=pad_fill),\n",
    "    # 의료: 강한 크롭/플립 제거, 약한 회전만\n",
    "    transforms.RandomApply([transforms.RandomRotation(5, interpolation=InterpolationMode.BILINEAR)], p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cfg['mean'], std=cfg['std']),\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    ResizeLongestSide(TARGET_SIZE),\n",
    "    PadToSquare(TARGET_SIZE, fill=pad_fill),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cfg['mean'], std=cfg['std']),\n",
    "])\n",
    "\n",
    "\n",
    "# ---------- datasets / loaders ----------\n",
    "train_ds = ImageFolder(TRAIN_DIR, transform=train_tf)\n",
    "val_ds   = ImageFolder(VAL_DIR,   transform=val_tf)\n",
    "\n",
    "\n",
    "print(\"train classes:\", train_ds.class_to_idx)\n",
    "print(\"val classes:\",   val_ds.class_to_idx)\n",
    "print(\"train counts:\",  Counter(train_ds.targets))\n",
    "print(\"val counts:\",    Counter(val_ds.targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor(train_ds.targets)\n",
    "class_counts = torch.bincount(targets, minlength=NUM_CLASSES).float()\n",
    "# 각 클래스 반비례 가중치(총합 정규화)\n",
    "weights = (class_counts.sum() / (class_counts + 1e-6)).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=weights,\n",
    "    label_smoothing=0.1,  # 작은 데이터에 도움\n",
    ").to(DEVICE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "\n",
    "# ---------- partial fine-tune (head + last stages) ----------\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "# head 활성화\n",
    "if hasattr(model, \"head\"):\n",
    "    for p in model.head.parameters():\n",
    "        p.requires_grad = True\n",
    "# 마지막 N개 stage 활성화\n",
    "if hasattr(model, \"stages\"):\n",
    "    total = len(model.stages)\n",
    "    start = max(0, total - UNFREEZE_STAGES)\n",
    "    for i in range(start, total):\n",
    "        for p in model.stages[i].parameters():\n",
    "            p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 파라미터 그룹 (head vs backbone)\n",
    "head_params, backbone_params = [], []\n",
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        if (\"head\" in n) or (\"classifier\" in n):\n",
    "            head_params.append(p)\n",
    "        else:\n",
    "            backbone_params.append(p)\n",
    "\n",
    "\n",
    "UNFREEZE_STAGES = 4  # 전부\n",
    "for p in model.parameters(): p.requires_grad = False\n",
    "for p in model.head.parameters(): p.requires_grad = True\n",
    "for i in range(len(model.stages) - UNFREEZE_STAGES, len(model.stages)):\n",
    "    for p in model.stages[i].parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": [p for n,p in model.named_parameters() if p.requires_grad and \"head\" not in n], \"lr\": 5e-5},\n",
    "    {\"params\": [p for n,p in model.named_parameters() if p.requires_grad and (\"head\" in n or \"classifier\" in n)], \"lr\": 3e-4},\n",
    "], weight_decay=0.01)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_train_steps = EPOCHS * len(train_loader)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_train_steps),\n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "scaler = torch.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 87690372\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------- training ----------\n",
    "print(\"trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] val_loss=1.2860  acc=0.3871\n",
      "[2] val_loss=1.3134  acc=0.3629\n",
      "[3] val_loss=1.5284  acc=0.2177\n",
      "[4] val_loss=1.2173  acc=0.5000\n",
      "[5] val_loss=1.3448  acc=0.4597\n",
      "[6] val_loss=1.4868  acc=0.4274\n",
      "[7] val_loss=1.5112  acc=0.4516\n",
      "[8] val_loss=1.7897  acc=0.4839\n",
      "[9] val_loss=2.5653  acc=0.4032\n",
      "[10] val_loss=2.6663  acc=0.4435\n",
      "[11] val_loss=1.5744  acc=0.5242\n",
      "[12] val_loss=2.2448  acc=0.4597\n",
      "[13] val_loss=2.2924  acc=0.3629\n",
      "[14] val_loss=2.0887  acc=0.4677\n",
      "[15] val_loss=2.5199  acc=0.5000\n",
      "[16] val_loss=2.0806  acc=0.5081\n",
      "[17] val_loss=2.6700  acc=0.4355\n",
      "[18] val_loss=2.5598  acc=0.4839\n",
      "[19] val_loss=2.3762  acc=0.5161\n",
      "⏹ Early stopped at epoch 19 (no improvement for 15 epochs).\n"
     ]
    }
   ],
   "source": [
    "# ==== EarlyStopping 파라미터 ====\n",
    "best_loss = float('inf')\n",
    "patience = 15         # 권장: 10~15\n",
    "min_delta = 1e-3      # 이만큼 내려가야 “개선”으로 인정\n",
    "no_improve = 0\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for step, (x, y) in enumerate(train_loader, 1):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        with torch.amp.autocast(device_type=DEVICE, enabled=(DEVICE==\"cuda\")):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()  # per-step 스케줄 유지\n",
    "\n",
    "        running += loss.item()\n",
    "        if step % 50 == 0:\n",
    "            print(f\"ep {ep} | step {step}/{len(train_loader)} | train_loss {running/50:.4f}\")\n",
    "            running = 0.0\n",
    "\n",
    "    val_loss, val_acc = evaluate()\n",
    "    print(f\"[{ep}] val_loss={val_loss:.4f}  acc={val_acc:.4f}\")\n",
    "\n",
    "    # 저장 & EarlyStopping 로직\n",
    "    if best_loss - val_loss > min_delta:\n",
    "        best_loss = val_loss\n",
    "        no_improve = 0\n",
    "        # torch.save(model.state_dict(), \"/camin1/chlee/jupyter/Keratopathy AI Project/[25-08-11]/convnextv2_base_384_cornea_best.pt\")\n",
    "        # print(\"💾 saved: convnextv2_base_384_cornea_best.pt\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"⏹ Early stopped at epoch {ep} (no improvement for {patience} epochs).\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RT_slit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train classes: {'0': 0, '1': 1, '2': 2, '3': 3}\n",
      "val classes: {'0': 0, '1': 1, '2': 2, '3': 3}\n",
      "train counts: Counter({2: 79, 0: 74, 3: 47, 1: 12})\n",
      "val counts: Counter({2: 55, 0: 45, 3: 27, 1: 13})\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"/camin1/chlee/Data/cornea_image/segmented/Rt/slit/split\")  # <- 본인 경로로 변경\n",
    "TRAIN_DIR    = DATA_DIR / \"train\"\n",
    "VAL_DIR      = DATA_DIR / \"val\"\n",
    "\n",
    "model = timm.create_model(MODEL_ID, pretrained=True, num_classes=NUM_CLASSES)\n",
    "\n",
    "# 안전하게 분류기 리셋\n",
    "if hasattr(model, \"reset_classifier\"):\n",
    "    model.reset_classifier(num_classes=NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "\n",
    "# ---------- transforms (LongestSide + square pad) ----------\n",
    "cfg = resolve_model_data_config(model)  # mean/std\n",
    "\n",
    "\n",
    "pad_fill = tuple(int(m*255) for m in cfg['mean'])  # 배경을 mean 색으로 패딩\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    ResizeLongestSide(TARGET_SIZE),\n",
    "    PadToSquare(TARGET_SIZE, fill=pad_fill),\n",
    "    # 의료: 강한 크롭/플립 제거, 약한 회전만\n",
    "    transforms.RandomApply([transforms.RandomRotation(5, interpolation=InterpolationMode.BILINEAR)], p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cfg['mean'], std=cfg['std']),\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    ResizeLongestSide(TARGET_SIZE),\n",
    "    PadToSquare(TARGET_SIZE, fill=pad_fill),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cfg['mean'], std=cfg['std']),\n",
    "])\n",
    "\n",
    "\n",
    "# ---------- datasets / loaders ----------\n",
    "train_ds = ImageFolder(TRAIN_DIR, transform=train_tf)\n",
    "val_ds   = ImageFolder(VAL_DIR,   transform=val_tf)\n",
    "\n",
    "\n",
    "print(\"train classes:\", train_ds.class_to_idx)\n",
    "print(\"val classes:\",   val_ds.class_to_idx)\n",
    "print(\"train counts:\",  Counter(train_ds.targets))\n",
    "print(\"val counts:\",    Counter(val_ds.targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor(train_ds.targets)\n",
    "class_counts = torch.bincount(targets, minlength=NUM_CLASSES).float()\n",
    "# 각 클래스 반비례 가중치(총합 정규화)\n",
    "weights = (class_counts.sum() / (class_counts + 1e-6)).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=weights,\n",
    "    label_smoothing=0.1,  # 작은 데이터에 도움\n",
    ").to(DEVICE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "\n",
    "# ---------- partial fine-tune (head + last stages) ----------\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "# head 활성화\n",
    "if hasattr(model, \"head\"):\n",
    "    for p in model.head.parameters():\n",
    "        p.requires_grad = True\n",
    "# 마지막 N개 stage 활성화\n",
    "if hasattr(model, \"stages\"):\n",
    "    total = len(model.stages)\n",
    "    start = max(0, total - UNFREEZE_STAGES)\n",
    "    for i in range(start, total):\n",
    "        for p in model.stages[i].parameters():\n",
    "            p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 파라미터 그룹 (head vs backbone)\n",
    "head_params, backbone_params = [], []\n",
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        if (\"head\" in n) or (\"classifier\" in n):\n",
    "            head_params.append(p)\n",
    "        else:\n",
    "            backbone_params.append(p)\n",
    "\n",
    "\n",
    "UNFREEZE_STAGES = 4  # 전부\n",
    "for p in model.parameters(): p.requires_grad = False\n",
    "for p in model.head.parameters(): p.requires_grad = True\n",
    "for i in range(len(model.stages) - UNFREEZE_STAGES, len(model.stages)):\n",
    "    for p in model.stages[i].parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": [p for n,p in model.named_parameters() if p.requires_grad and \"head\" not in n], \"lr\": 5e-5},\n",
    "    {\"params\": [p for n,p in model.named_parameters() if p.requires_grad and (\"head\" in n or \"classifier\" in n)], \"lr\": 3e-4},\n",
    "], weight_decay=0.01)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_train_steps = EPOCHS * len(train_loader)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_train_steps),\n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "scaler = torch.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 87690372\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------- training ----------\n",
    "print(\"trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] val_loss=1.2960  acc=0.3929\n",
      "[2] val_loss=1.3192  acc=0.3143\n",
      "[3] val_loss=1.3245  acc=0.4000\n",
      "[4] val_loss=1.2496  acc=0.4071\n",
      "[5] val_loss=1.6755  acc=0.3357\n",
      "[6] val_loss=1.4449  acc=0.4286\n",
      "[7] val_loss=1.8047  acc=0.4429\n",
      "[8] val_loss=1.9421  acc=0.3357\n",
      "[9] val_loss=1.9782  acc=0.4643\n",
      "[10] val_loss=2.5735  acc=0.4000\n",
      "[11] val_loss=1.8575  acc=0.4286\n",
      "[12] val_loss=2.4209  acc=0.3929\n",
      "[13] val_loss=2.5187  acc=0.4071\n",
      "[14] val_loss=2.5972  acc=0.4357\n",
      "[15] val_loss=2.9844  acc=0.3929\n",
      "[16] val_loss=3.0898  acc=0.3643\n",
      "[17] val_loss=2.6993  acc=0.4357\n",
      "[18] val_loss=2.8806  acc=0.4214\n",
      "[19] val_loss=2.9906  acc=0.3857\n",
      "⏹ Early stopped at epoch 19 (no improvement for 15 epochs).\n"
     ]
    }
   ],
   "source": [
    "# ==== EarlyStopping 파라미터 ====\n",
    "best_loss = float('inf')\n",
    "patience = 15         # 권장: 10~15\n",
    "min_delta = 1e-3      # 이만큼 내려가야 “개선”으로 인정\n",
    "no_improve = 0\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for step, (x, y) in enumerate(train_loader, 1):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        with torch.amp.autocast(device_type=DEVICE, enabled=(DEVICE==\"cuda\")):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()  # per-step 스케줄 유지\n",
    "\n",
    "        running += loss.item()\n",
    "        if step % 50 == 0:\n",
    "            print(f\"ep {ep} | step {step}/{len(train_loader)} | train_loss {running/50:.4f}\")\n",
    "            running = 0.0\n",
    "\n",
    "    val_loss, val_acc = evaluate()\n",
    "    print(f\"[{ep}] val_loss={val_loss:.4f}  acc={val_acc:.4f}\")\n",
    "\n",
    "    # 저장 & EarlyStopping 로직\n",
    "    if best_loss - val_loss > min_delta:\n",
    "        best_loss = val_loss\n",
    "        no_improve = 0\n",
    "        # torch.save(model.state_dict(), \"/camin1/chlee/jupyter/Keratopathy AI Project/[25-08-11]/convnextv2_base_384_cornea_best.pt\")\n",
    "        # print(\"💾 saved: convnextv2_base_384_cornea_best.pt\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"⏹ Early stopped at epoch {ep} (no improvement for {patience} epochs).\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RT_slit_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train classes: {'0': 0, '1': 1, '2': 2, '3': 3}\n",
      "val classes: {'0': 0, '1': 1, '2': 2, '3': 3}\n",
      "train counts: Counter({2: 63, 0: 60, 3: 43, 1: 10})\n",
      "val counts: Counter({2: 49, 0: 41, 3: 22, 1: 12})\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"/camin1/chlee/Data/cornea_image/segmented/Rt/slit_beam/split\")  # <- 본인 경로로 변경\n",
    "TRAIN_DIR    = DATA_DIR / \"train\"\n",
    "VAL_DIR      = DATA_DIR / \"val\"\n",
    "\n",
    "model = timm.create_model(MODEL_ID, pretrained=True, num_classes=NUM_CLASSES)\n",
    "\n",
    "# 안전하게 분류기 리셋\n",
    "if hasattr(model, \"reset_classifier\"):\n",
    "    model.reset_classifier(num_classes=NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "\n",
    "# ---------- transforms (LongestSide + square pad) ----------\n",
    "cfg = resolve_model_data_config(model)  # mean/std\n",
    "\n",
    "\n",
    "pad_fill = tuple(int(m*255) for m in cfg['mean'])  # 배경을 mean 색으로 패딩\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    ResizeLongestSide(TARGET_SIZE),\n",
    "    PadToSquare(TARGET_SIZE, fill=pad_fill),\n",
    "    # 의료: 강한 크롭/플립 제거, 약한 회전만\n",
    "    transforms.RandomApply([transforms.RandomRotation(5, interpolation=InterpolationMode.BILINEAR)], p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cfg['mean'], std=cfg['std']),\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    ResizeLongestSide(TARGET_SIZE),\n",
    "    PadToSquare(TARGET_SIZE, fill=pad_fill),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cfg['mean'], std=cfg['std']),\n",
    "])\n",
    "\n",
    "\n",
    "# ---------- datasets / loaders ----------\n",
    "train_ds = ImageFolder(TRAIN_DIR, transform=train_tf)\n",
    "val_ds   = ImageFolder(VAL_DIR,   transform=val_tf)\n",
    "\n",
    "\n",
    "print(\"train classes:\", train_ds.class_to_idx)\n",
    "print(\"val classes:\",   val_ds.class_to_idx)\n",
    "print(\"train counts:\",  Counter(train_ds.targets))\n",
    "print(\"val counts:\",    Counter(val_ds.targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor(train_ds.targets)\n",
    "class_counts = torch.bincount(targets, minlength=NUM_CLASSES).float()\n",
    "# 각 클래스 반비례 가중치(총합 정규화)\n",
    "weights = (class_counts.sum() / (class_counts + 1e-6)).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=weights,\n",
    "    label_smoothing=0.1,  # 작은 데이터에 도움\n",
    ").to(DEVICE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "\n",
    "# ---------- partial fine-tune (head + last stages) ----------\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "# head 활성화\n",
    "if hasattr(model, \"head\"):\n",
    "    for p in model.head.parameters():\n",
    "        p.requires_grad = True\n",
    "# 마지막 N개 stage 활성화\n",
    "if hasattr(model, \"stages\"):\n",
    "    total = len(model.stages)\n",
    "    start = max(0, total - UNFREEZE_STAGES)\n",
    "    for i in range(start, total):\n",
    "        for p in model.stages[i].parameters():\n",
    "            p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 파라미터 그룹 (head vs backbone)\n",
    "head_params, backbone_params = [], []\n",
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        if (\"head\" in n) or (\"classifier\" in n):\n",
    "            head_params.append(p)\n",
    "        else:\n",
    "            backbone_params.append(p)\n",
    "\n",
    "\n",
    "UNFREEZE_STAGES = 4  # 전부\n",
    "for p in model.parameters(): p.requires_grad = False\n",
    "for p in model.head.parameters(): p.requires_grad = True\n",
    "for i in range(len(model.stages) - UNFREEZE_STAGES, len(model.stages)):\n",
    "    for p in model.stages[i].parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": [p for n,p in model.named_parameters() if p.requires_grad and \"head\" not in n], \"lr\": 5e-5},\n",
    "    {\"params\": [p for n,p in model.named_parameters() if p.requires_grad and (\"head\" in n or \"classifier\" in n)], \"lr\": 3e-4},\n",
    "], weight_decay=0.01)\n",
    "\n",
    "\n",
    "num_train_steps = EPOCHS * len(train_loader)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_train_steps),\n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "scaler = torch.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 87690372\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------- training ----------\n",
    "print(\"trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chlee/.conda/envs/keratopathy/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] val_loss=1.2931  acc=0.2661\n",
      "[2] val_loss=1.3154  acc=0.3306\n",
      "[3] val_loss=1.1804  acc=0.4839\n",
      "[4] val_loss=1.2042  acc=0.4597\n",
      "[5] val_loss=1.4347  acc=0.3790\n",
      "[6] val_loss=1.3659  acc=0.5161\n",
      "[7] val_loss=1.6286  acc=0.4274\n",
      "[8] val_loss=1.6547  acc=0.5403\n",
      "[9] val_loss=2.1419  acc=0.5323\n",
      "[10] val_loss=2.4690  acc=0.4597\n",
      "[11] val_loss=1.9957  acc=0.4839\n",
      "[12] val_loss=1.9168  acc=0.5000\n",
      "[13] val_loss=2.1385  acc=0.5726\n",
      "[14] val_loss=2.0271  acc=0.4597\n",
      "[15] val_loss=2.3311  acc=0.4839\n",
      "[16] val_loss=2.8440  acc=0.4194\n",
      "[17] val_loss=2.0318  acc=0.5081\n",
      "[18] val_loss=2.3728  acc=0.4516\n",
      "⏹ Early stopped at epoch 18 (no improvement for 15 epochs).\n"
     ]
    }
   ],
   "source": [
    "# ==== EarlyStopping 파라미터 ====\n",
    "best_loss = float('inf')\n",
    "patience = 15         # 권장: 10~15\n",
    "min_delta = 1e-3      # 이만큼 내려가야 “개선”으로 인정\n",
    "no_improve = 0\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for step, (x, y) in enumerate(train_loader, 1):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        with torch.amp.autocast(device_type=DEVICE, enabled=(DEVICE==\"cuda\")):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()  # per-step 스케줄 유지\n",
    "\n",
    "        running += loss.item()\n",
    "        if step % 50 == 0:\n",
    "            print(f\"ep {ep} | step {step}/{len(train_loader)} | train_loss {running/50:.4f}\")\n",
    "            running = 0.0\n",
    "\n",
    "    val_loss, val_acc = evaluate()\n",
    "    print(f\"[{ep}] val_loss={val_loss:.4f}  acc={val_acc:.4f}\")\n",
    "\n",
    "    # 저장 & EarlyStopping 로직\n",
    "    if best_loss - val_loss > min_delta:\n",
    "        best_loss = val_loss\n",
    "        no_improve = 0\n",
    "        # torch.save(model.state_dict(), \"/camin1/chlee/jupyter/Keratopathy AI Project/[25-08-11]/convnextv2_base_384_cornea_best.pt\")\n",
    "        # print(\"💾 saved: convnextv2_base_384_cornea_best.pt\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"⏹ Early stopped at epoch {ep} (no improvement for {patience} epochs).\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keratopathy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
